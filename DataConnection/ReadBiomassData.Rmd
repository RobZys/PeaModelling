---
title: "Transforming Pea data into observed data for sims discovery science"
author: "R Zyskowski , Adrain Hunt"
date: "`r Sys.Date()`"
output:
  github_document:
    toc: yes
  html_document: default
    
---

## Step one read data from Excel File

Key processes:

- Reads  *Technically correct* data into `data.frame`

- Combines the Lincoln & Hawkes bay data


#### Data management

```{r, results='hide', echo=FALSE, include=FALSE}

rm(list=setdiff(ls(), "source_rmd")) #remove all objects but "source_rmd" from the environment before assigning the new objects .. this is especially included to use this script as a child script in the MainScript.Rmd file 

#rm(list = ls(all = TRUE))

```

Load the required libraries.


```{r, warning=FALSE}
# load the required libraries
library(lubridate)
library(plyr);library(dplyr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(magrittr) #required to use the pipe (%>%) operator
library(readxl)
library(tidyverse)

```


** Set the directories and Data File**
```{r}
url <- "https://iplant.plantandfood.co.nz/project/P442060-13/_layouts/15/WopiFrame.aspx?sourcedoc=/project/P442060-13/Research/18_19_SAE_pea_protein_managment.xlsx&action=default"

Source_HB = "RAW_HB"
Source_Linc = "RAW_Lincoln"

```





### Set up function to read Reads *RAW* (pre-processed) data into a `data.frame`

```{r}

library(readxl)
library(tidyverse)

scrape_xl <- function(url = NULL, sheet = NULL, skip = NULL) {
  
  # Ensure the input url is a character vector.
  stopifnot(is.character(url))
  
  # Check that the input url has the correct form.
  if (!grepl("layout", url) && !grepl(".xl", url)) {
    stop("Url is not of the correct form")
  }
  
  # Extract the base of the url.
  base_url <- url %>%
    stringr::str_split("_layouts") %>%
    lapply(`[[`, 1) %>%
    unlist()
  
  # Extract the extension of the url and replace spaces with encoded format.
  extension <- url %>%
    stringr::str_split("=/") %>%
    lapply(`[[`, 2) %>%
    unlist() %>%
    stringr::str_split(".xlsx") %>%
    lapply(`[[`, 1) %>%
    unlist() %>%
    stringr::str_replace_all(" ", "%20")
  
  # Define the correct file format to be added to the url.
  file_format <- "_layouts/download.aspx?SourceUrl=/"
  
  # Paste the base, format and extension together.
  url_fmt <- paste0(paste(base_url, extension, sep = file_format), ".xlsx")
  
  # Check that the formatted url exists and get the user credentials.
  if (httr::http_error(httr::GET(url_fmt)) == 404L) {
    stop("Url does not exist")
  }
  username <- Sys.getenv("USER")
  if (Sys.getenv("PASSWORD") == "") {
    Sys.setenv("PASSWORD" = getPass::getPass(paste0(username,
                                                    ", enter your password")))
  }
  password <- Sys.getenv("PASSWORD")
  
  # Check that the reponse had no errors with the user credentials.
  if (httr::http_error(httr::GET(url_fmt,
                                 httr::authenticate(username,
                                                    password,
                                                    type = "ntlm")))) {
    stop("The request failed, check username and password",
         httr::http_status(res))
  }
  
  # Get the response to the Excel spreadsheet and write the data to a
  # temporary file.
  httr::GET(url_fmt,
            httr::authenticate(username, password, type = "ntlm"),
            httr::write_disk(temp_file <- tempfile(fileext = ".xlsx")))
  
  # Read the Excel data in from the temp file to a dataframe.
  df <- readxl::read_excel(temp_file, sheet = sheet, skip = skip)
  
  # Return the Excel spreadsheet data from iPlant as a dataframe.
  return(df)
}

```

### Create the Raw Data files


```{r}

df_RAW_Lincoln<- scrape_xl(url, sheet = Source_Linc, skip = 2)
df_RAW_HB<- scrape_xl(url, sheet =  Source_HB, skip = 2)


df_RAW_HB$Index  <- NULL
df_RAW_Lincoln$Index  <- NULL

```

#### Inspection of loaded data set

```{r}
#Retrieve the classes of all columns in a data.frame 
str(df_RAW_Lincoln)
```



```{r}
str(df_RAW_HB)
```



```{r}
summary(df_RAW_HB)
```





```{r}
head(df_RAW_HB)
```


### Removing rows with NAs (missing values) in `data.frame`

```{r}
# Remove rows with NAs in data.frame
#df_HBData <- df_RAW_HB[!complete.cases(df_RAW_HB), ]
df_HBData <- df_RAW_HB
na.omit(df_HBData)

df_LincolnData <- df_RAW_Lincoln
na.omit(df_LincolnData)

#head(df_HBData)

```

### Add column Represeint Site
```{r}
df_HBData$Site = "HawkesBay"
df_LincolnData$Site = "Lincoln"

```
```{r}
df_HBData$partFW <- as.numeric(as.character(df_HBData$partFW)) 
df_HBData$maturepodFW <- as.numeric(as.character(df_HBData$maturepodDW)) 
df_LincolnData$maturepodFW <- as.numeric(as.character(df_LincolnData$maturepodDW)) 


```

### Merge the Two Dataframes based on Common factors
```{r}
df_CombinedData  <- bind_rows(df_HBData,df_LincolnData)

head(df_CombinedData)

```



